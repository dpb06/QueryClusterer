%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Catagory Clustering for Query Classification}
\author{David Blackford\thanks{Thanks to Xiaoxing Gao for your help, It was much appreciated}\\
Victoria University of Wellington\\
Kelburn Parade\\
Wellington\\
New Zealand\\
}
\maketitle
\begin{abstract}
\begin{quote}
Short Query classification is an work area that relies on the enrichment of short queries in order to get enough information to successfully classify the query into 
one of many catagories. This task can be done in many ways, and this paper discusses my attempt at a clustering algorithm designed to catagorize a query based on the results of search engines.
\end{quote}
\end{abstract}
\section{Introduction}

\section{Related Work}



\section{Algorithm}
The algorithm designed for this paper is based on the work \cite{distCluster} and attempts to find the most common distribution of catagories for words that are related
to a given query. The query begins unenriched, and is enriched through use of the Google search engine(http://www.google.com). The resulting body of words is taken
as related to the query, and these words are used to find a distibution of the algorithms relative belief that the query should belong to a given catagory/catagories.
Each word for the enriched query measured against each of the given catagories using one of many possible measures. Once we have measured the rough probability distribution of the word
belonging to the different catagories, we have to decide on which of these distributions comes closest to representing the query which the words are enriching. 

In this algorithm we do so by, rather than selecting a distribution, by clustering similar distributions until one distribution becomes a fair representation of the majority of the distributions
we began with. This is completed throught use of the Kullbeck-Leibler algorithm \cite{kullbeckLeibler} which is an algorithm which calculates a measure, the dissimilarity, between two given distributions. If two distributions are similar, then the dissimilarity value returned by the Kullbeck-Leibler algorithm will be extremely low. If two distributions are completely unlike each other, then the dissimilarity values in both directions will be low. 

\subsection{Issues with the algorithm design}
One issue with the Kullbeck-Leibler algorithm is that it is a bi-directional algorithm, that returns different values in each directions in most cases. 
It is described in the work by \cite{distCluster} as an algorithm which describes how well messages intended for one distribution fare when they are sent to the other distribution, which is a process which will be different depending on which distribution the messages were originally intended for. 
This leads to different values depending on the order of the distributions. 
In this paper they work around this issue by averaging the message based on how often a word appears in the category. 
In the algorithm that I am working on I have gone with a much simpler pure average, much like the one used in their earlier paper \cite{distClusterEarlier}.
Despite this there are still issues with using the Kullbeck-Leiber algorithm. For example, when one distribution's catagory has a very low probability, and the other distribution has a very high probability, the dissimilarity reaches towards infinity, which is most prominent as an issue when one distribution has a probability of zero for a particular category and the other does not. This case, where one value is zero and one is not, is an occurance which cant be over looked. In these cases the difference will be infinite, an issue which the writers of the paper didn't discuss their solution to. In my case, I ensure that there is a large penalty value attributed to this, one which will be hard to overcome in order to become the most similar distribution to one you disagree with, but only by having less of these situations occuring than the other. This change may be one that cripples the algorithm, causing any cases where this occurs to lean too heavily towards not having any points where one is zero and the other is not despite all other sections of the distributions being identical. This however was a situation where it is possible that the only true solution was ensuring that there was some, possibly minuiscule probability of being any given category.
Balancing this could become a major problem, in either solution. 



\section{Implementation}

Written in java etc
The measure that was originally attempted for this algorithm was the relatedness measure found from Wikiminer \cite{wikiminer} however the implementation of the Wikiminer program was poorly documented, and the relatedness measure relied on a webservice that seemed to be shut-down. Another measure that was attempted was use of Normalized Google Distance \cite{NGD}. This is a measure which makes use of the 
Google search engine's approximate pages in order to calculate how often two words appear together relative to how often they appear apart. This measure seems extremely promising, however
Google places limits on the number of queries and frequency of queries. For initial 


\section{Experimentation

\section{Future Work}

\section{Conclusion}


%\bibliographystyle{aaai}
%\bibliography{bibfile1}
\end{document}
