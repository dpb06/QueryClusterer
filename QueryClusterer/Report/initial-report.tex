%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Catagory Clustering for Query Classification}
\author{David Blackford\thanks{Thanks to Xiaoxing Gao for your help, It was much appreciated}\\
Victoria University of Wellington\\
Kelburn Parade\\
Wellington\\
New Zealand\\
}
\maketitle
\begin{abstract}
\begin{quote}
Short Query classification is an work area that relies on the enrichment of short queries in order to get enough information to successfully classify the query into 
one of many catagories. This task can be done in many ways, and this paper discusses my attempt at a clustering algorithm designed to catagorize a query based on the results of search engines.
\end{quote}
\end{abstract}
\section{Introduction}

\section{Related Work}



\section{Algorithm}
The algorithm designed for this paper is based on the work \cite{distCluster} and attempts to find the most common distribution of catagories for words that are related
to a given query. The query begins unenriched, and is enriched through use of the Google search engine(http://www.google.com). The resulting body of words is assumed
to be related to the query in some form, and so these words are used to find a distibution of the algorithms relative belief that the query should belong to a given catagory/catagories.


The intended benefits of this approach is an attempt to determine the category of the query by the majority vote of the related words. This does not occur as is discussed in the Experimentation section, however this is mainly because many of the words returned are generic words used in all categories and should possibly be included in stopwords, with only a few specific words carrying a large probability for any given category. 


Each word for the enriched query measured against each of the given catagories using one of many possible measures. Once we have measured the rough probability distribution of the word
belonging to the different catagories, we have to decide on which of these distributions comes closest to representing the query which the words are enriching. 

In this algorithm we do so by, rather than selecting a distribution, clustering similar distributions until one distribution becomes a fair representation of the majority of the distributions
we began with. This is completed throught use of the Kullbeck-Leibler algorithm \cite{kullbeckLeibler} which is an algorithm which calculates a measure, the dissimilarity, between two given distributions. If two distributions are similar, then the dissimilarity value returned by the Kullbeck-Leibler algorithm will be extremely low. If two distributions are completely unlike each other, then the dissimilarity values in both directions will be low. 

Once each word has a probability distribution, the strongest distributions are selected to seed the clusters, 

\subsection{Issues with the algorithm design}
One issue with the Kullbeck-Leibler algorithm is that it is a bi-directional algorithm, that returns different values in each directions in most cases. 
It is described in the work by \cite{distCluster} as an algorithm which describes how well messages intended for one distribution fare when they are sent to the other distribution, which is a process which will be different depending on which distribution the messages were originally intended for. 
This leads to different values depending on the order of the distributions. 
In this paper they work around this issue by averaging the message based on how often a word appears in the category. 
In the algorithm that I am working on I have gone with a much simpler pure average, much like the one used in their earlier paper \cite{distClusterEarlier}.


Despite this there are still issues with using the Kullbeck-Leiber algorithm. For example, when one distribution's catagory has a very low probability, and the other distribution has a very high probability, the dissimilarity reaches towards infinity, which is most prominent as an issue when one distribution has a probability of zero for a particular category and the other does not. This case, where one value is zero and one is not, is an occurance which cant be over looked. In these cases the dissimilarity will be infinite, an issue which the writers of the paper didn't discuss their solution to. In my case, I ensure that there is a large penalty value attributed to this which will be difficult but not impossible to overcome in order to become a distribution others agree with, but only by having less of these situations occuring than the other. This change may be one that cripples the algorithm, causing any cases where this occurs to lean too heavily towards not having any points where one is zero and the other is not. If the catagory for which this occurs is completely unrelated to the document as a whole, and possibly dissagree with the rest of the distribution, it could cause that distribution to not pair with any of the other distribution, whether they too are unlikely to be placed in that category or not. This however was a situation where it is possible that the only realistic solution was ensuring that there was some, possibly minuiscule probability of being any given category. Balancing this could become a major problem, in either solution, but it is something that would need to be considered in order to improve this algorithm further. 



\section{Implementation}

The implementation of the query clusterer was written in Java and made use of Bing as a service to compare Word Enrichments to Categories. 
The measure that was originally attempted for this algorithm was the relatedness measure found from Wikiminer \cite{wikiminer} however the implementation of the Wikiminer program was poorly documented, and the relatedness measure relied on a webservice that seemed to be shut-down. Another measure that was attempted was use of Normalized Google Distance(NGD) \cite{NGD}. This is a measure which makes use of the 
Google search engine's approximate pages in order to calculate how often two words appear together relative to how often they appear apart. This measure seems extremely promising, however
Google places limits on the number of queries and frequency of queries. As a proof of concept though I feel that the NGD distance would be sufficient, and show the merits of the distribusion clustering algorithm. Given the earlier discussed issues with the algorithm, NGD would only result in problems when one of the enrichment's returns 0 results when searched with one of the categories. This would result in the Kullbeck-Leibler algorithm returning an infinite dissimilarity between it and any other distribution. To avoid this situation, in the case where there is a 0 probability for one of the enrichments belonging to a category, rather than resulting in an infinite dissimilarity, I limit the dissimilarity to a much smaller (than infinity) penalty value. This still results in an extremely high dissimilarty, however it no longer causes issues in the comparison of distributions and clustering.

One of the important parts of this implementation is that all data is stored given the chance, so once it has been worked out once, it is simply a case of accessing the data again rather than recalculating.
This improves the computation time of the algorithm immensely, but also ensures that it will scale acceptably when run on larger datasets.



\section{Experimentation}

While I was able to get the algorithm working, I am as of yet unsure of it's success.
Because of issues discussed earlier in finding a rich body of enrichment and measurement for each query and related words, I feel that while my algorithm shows promising results
on the individual runs I have completed for different catagories, I do not have a convincing proof of the algorithm's strengths.

The individual runs I have completed on 2 catagories run through the algorithm, despite a large amount of remaining noise in the dataset, were reasonably successful.

Issues that I found in implementation were that it became a problem with not knowing how to select the cluster that I was going to choose to represent the query.
as an example, my run on the query "0apr" resulted in two clusters containing only single words. These words were "apr" and "credit" so are extremely related
to the query, however they were so distinct in their levels of relation that no other words or clusters deemed them close enough to cluster to. My initial reaction
is to use the distribution attributed to the word with the highest average distribution probability, "credit" however while this is successful in picking out apr and credit
as the two highest clusters it could quite easily have been "apr" as the only distribution returned at the end of the algorithm, in which case the information gained could be reasonably small
when compared with "credit" as a result. 
This simple measure of which distribution could have many more issues when dealing with clusters with very broad but moderately thick distributions, which could have a much higher average despite
the words contained within having only very broad and generic catagory prefrences. 
Despite this, I was happy with the results given the very basic dataset that I ran the
algorithm on. Using a search engine to get a basic Normalized "Bing" Distance as a simple measure of relatedness for the different values worked reasonably well
but would need a full Enrichment to Catagory comparison to be done in order to fully and successfully.
With the algorithm averaging out the dataset and clustering the similar distributions led to distributions that removed much of the noise and only kept related and relevant words
in the final cluster. 

\section{Future Work}

This implementation was evaluated with very simple and barebones datasets, so there is very little that
can be said about it's success with confidence. To evaluate this algorithm properly, I would have to have
a full dataset, with NGD's for each combination of Word to Category for a decent sized enrichment of each Query.

Based on this I would be able to run the algorithm on each query and arrive at categorizations that could be compared
with the original training data using the precision/recall used to evaluate categorization algorithms,


\section{Conclusion}


%\bibliographystyle{aaai}
%\bibliography{bibfile1}
\end{document}
